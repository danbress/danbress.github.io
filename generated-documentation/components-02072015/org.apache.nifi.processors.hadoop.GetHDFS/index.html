<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"></meta><title>GetHDFS</title><link rel="stylesheet" href="../../css/component-usage.css" type="text/css"></link></head><body><h2>Description: </h2><p>Fetch files from Hadoop Distributed File System (HDFS) into FlowFiles</p><p><a href="additionalDetails.html">Additional Details...</a></p><h3>Tags: </h3><p>hadoop, HDFS, get, fetch, ingest, source, filesystem</p><h3>Properties: </h3><p>In the list below, the names of required properties appear in <strong>bold</strong>. Any other properties (not in bold) are considered optional. The table also indicates any default values, whether a property supports the <a href="../../html/expression-language-guide.html">NiFi Expression Language</a>, and whether a property is considered "sensitive", meaning that its value will be encrypted. Before entering a value in a sensitive property, ensure that the <strong>nifi.properties</strong> file has an entry for the property <strong>nifi.sensitive.props.key</strong>.</p><table><tr><th>Name</th><th>Default Value</th><th>Valid Values</th><th>Description</th></tr><tr><td>Hadoop Configuration Resources</td><td></td><td></td><td>A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a 'core-site.xml' and 'hdfs-site.xml' file or will revert to a default configuration.</td></tr><tr><td><strong>Directory</strong></td><td></td><td></td><td>The HDFS directory from which files should be read</td></tr><tr><td><strong>Recurse Subdirectories</strong></td><td>true</td><td><ul><li>true</li><li>false</li></ul></td><td>Indicates whether to pull files from subdirectories of the HDFS directory</td></tr><tr><td><strong>Keep Source File</strong></td><td>false</td><td><ul><li>true</li><li>false</li></ul></td><td>Determines whether to delete the file from HDFS after it has been successfully transferred</td></tr><tr><td>File Filter Regex</td><td></td><td></td><td>A Java Regular Expression for filtering Filenames; if a filter is supplied then only files whose names match that Regular Expression will be fetched, otherwise all files will be fetched</td></tr><tr><td><strong>Filter Match Name Only</strong></td><td>true</td><td><ul><li>true</li><li>false</li></ul></td><td>If true then File Filter Regex will match on just the filename, otherwise subdirectory names will be included with filename in the regex comparison</td></tr><tr><td><strong>Ignore Dotted Files</strong></td><td>true</td><td><ul><li>true</li><li>false</li></ul></td><td>If true, files whose names begin with a dot (".") will be ignored</td></tr><tr><td><strong>Minimum File Age</strong></td><td>0 sec</td><td></td><td>The minimum age that a file must be in order to be pulled; any file younger than this amount of time (based on last modification date) will be ignored</td></tr><tr><td>Maximum File Age</td><td></td><td></td><td>The maximum age that a file must be in order to be pulled; any file older than this amount of time (based on last modification date) will be ignored</td></tr><tr><td><strong>Polling Interval</strong></td><td>0 sec</td><td></td><td>Indicates how long to wait between performing directory listings</td></tr><tr><td><strong>Batch Size</strong></td><td>100</td><td></td><td>The maximum number of files to pull in each iteration, based on run schedule.</td></tr><tr><td>IO Buffer Size</td><td></td><td></td><td>Amount of memory to use to buffer file contents during IO. This overrides the Hadoop Configuration</td></tr></table><h3>Relationships: </h3><table><tr><th>Name</th><th>Description</th></tr><tr><td>passthrough</td><td>If this processor has an input queue for some reason, then FlowFiles arriving on that input are transferred to this relationship</td></tr><tr><td>success</td><td>All files retrieved from HDFS are transferred to this relationship</td></tr></table></body></html>